{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARMA10 task\n",
    "\n",
    "This task consists in predicting the output of a 10-th order non-linear autoregressive moving average (NARMA) system.\n",
    "- [Reference paper](https://doi.org/10.1016/j.neunet.2011.02.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import cuda, zeros, Tensor, optim\n",
    "from torch.nn import Module, ModuleList, Sequential, Linear, Tanh, MSELoss, RNN\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from grid import GS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the system is a sequence of elements 洧논(洧노) randomly chosen according to a uniform distribution over [0, 0.5]. \n",
    "\n",
    "Given the input value 洧논(洧노), the task is to predict the corresponding value of 洧녽(洧노).\n",
    "\n",
    "- Import the dataset from the .csv file *NARMA10.csv*, where the first row represents the input and the second row represents the\n",
    "target output. Different columns represent different time-steps.\n",
    "- Split the data into training (the first 5000 time steps), and test set (remaining time steps). Note that for model selection you will use the data in the training set, with a further split in training (first 4000 samples) and validation (last 1000 samples).\n",
    "    - For the sake of problem understanding, you can try to first visualize the timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = pd.read_csv('../data/NARMA10.csv', header=None).to_numpy()\n",
    "x = dset[0]\n",
    "y = dset[1]\n",
    "x_test = x[:5000]\n",
    "train = x[5000:] \n",
    "\n",
    "x_train = train[:4000]\n",
    "x_val = train[4000:]\n",
    "\n",
    "y_test = y[:5000]\n",
    "train_y = y[5000:] \n",
    "\n",
    "y_train = train_y[:4000]\n",
    "y_val = train_y[4000:]\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_ts(x_train,x_val,x_test, \"NARMA10 input values\")\n",
    "\n",
    "limit1=int(x_train.shape[0])\n",
    "limit2=int(x_train.shape[0]+x_val.shape[0])\n",
    "limit3=int(limit2+x_test.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12,5), dpi=200)\n",
    "plt.plot(range(0,limit1), x_train, color=\"indigo\", linewidth=0.2, label=\"training set\") \n",
    "plt.plot(range(limit1,limit2), x_val, color=\"purple\", linewidth=0.2, label=\"validation set\") \n",
    "plt.plot(range(limit2,limit3), x_test, color=\"violet\", linewidth=0.2, label=\"test set\") \n",
    "\n",
    "plt.title('NARMA10 input values')\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.grid(color = 'green', linestyle = '--', linewidth = 0.2)\n",
    "#plt.savefig(str('plots/timeseries.jpeg'), edgecolor='black', dpi=400, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:** when training an RNN you want to make sure to keep the last hidden state of your RNN after the training session, and use it as initial hidden state of the validation session.\n",
    "Same applies when transitioning to the test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class `GS` implements a grid search algorithm to find the best model configuration for either a\n",
    "# TDNN or RNN neural network based on specified parameters and datasets.\n",
    "class GS:\n",
    "    \n",
    "    def __init__(self, parameters:dict, Xset, Yset, neuralnet:str):\n",
    "        self.Xset=Xset\n",
    "        self.Yset=Yset\n",
    "        param_grid = self.grid(parameters) #self, \n",
    "        min_loss, best_model = self.search(param_grid, neuralnet) #self, \n",
    "        self.min_loss = min_loss\n",
    "        self.best_model = best_model # the best model discovered\n",
    "\n",
    "    # @staticmethod\n",
    "    def grid(self, params):\n",
    "        param_names=list(params.keys())\n",
    "        param_values=list(params.values())\n",
    "        param_combinations=list(itertools.product(*param_values))\n",
    "        \n",
    "        param_grid=[]\n",
    "        for combination in param_combinations:\n",
    "            param_grid.append(dict(zip(param_names, combination)))\n",
    "        return param_grid \n",
    "        \n",
    "    @staticmethod\n",
    "    def get_optimizer(model, optimizer_name, lr):\n",
    "        if optimizer_name == 'adam':\n",
    "            return optim.Adam(model.parameters(), lr=lr)\n",
    "        elif optimizer_name == 'sgd':\n",
    "            return optim.SGD(model.parameters(), lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer: {}\".format(optimizer_name))\n",
    "        \n",
    "    # @staticmethod\n",
    "    def search(self, param_grid:tuple, neuralnet:str):#, Xset, Yset):\n",
    "        predictions={}  \n",
    "        for pg in param_grid:\n",
    "            print(f'Parameters: {pg}')\n",
    "            if neuralnet=='TDNN':\n",
    "                model=TDNN_model(\n",
    "                    window=pg['window'],\n",
    "                    layers=pg['layers'],\n",
    "                    input_dim=self.Xset.shape[0],\n",
    "                    epochs=pg['epochs'],\n",
    "                    hidden_dim=pg['hiddens']       \n",
    "                )\n",
    "            elif neuralnet=='RNN':\n",
    "                model=RNN_model(\n",
    "                    input_dim=self.Xset.size(0), #self.Xset.shape[1]\n",
    "                    layers=pg['hiddens'],\n",
    "                    epochs=pg['epochs'],\n",
    "                    hidden_dim=10\n",
    "                ) \n",
    "            optimizer = self.get_optimizer(model, pg['opt'], pg['lr'])      \n",
    "            loss, y_pred=model.train(self.Xset, self.Yset, optimizer)#\n",
    "            predictions[loss]=[pg, y_pred] \n",
    "        \n",
    "        \n",
    "        # predictions=dict(sorted(predictions.items()))\n",
    "        # print(next(iter(predictions.items())))\n",
    "        # min_loss=list(predictions.keys())[0]\n",
    "        min_loss = min(predictions.keys())\n",
    "        # best_model=list(predictions.values())[0]\n",
    "        best_model = predictions[min_loss]\n",
    "        \n",
    "        return min_loss, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this function, you obtain a 1x[window_size]x4000 Torch Tensor \n",
    "# to pass to the TDNN as preprocessed input data\n",
    "\n",
    "def input_prep(window:int, input_data:np.array):\n",
    "        input_len=len(input_data)\n",
    "        prep_input=torch.zeros([1, input_len, window])\n",
    "        for i in range(0, input_len):\n",
    "            #if((i+window) <= input_len): \n",
    "                tmp=input_data[i:i+window]\n",
    "                #prep_input[0,i,:]=tmp \n",
    "                for j in tmp:\n",
    "                    prep_input[0,i,:]=j\n",
    "        # print('TENSOR:', prep_input.shape)\n",
    "        return prep_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(tr_y, tr_y_pred, ts_y, ts_y_pred, nn):\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.suptitle(nn)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(tr_y, label=\"True\",linewidth=0.2)\n",
    "    plt.plot(tr_y_pred, label=\"Predicted\", linewidth=0.2)\n",
    "    plt.title('Training Data')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(ts_y, label=\"True\", linewidth=0.2)\n",
    "    plt.plot(ts_y_pred, label=\"Predicted\", linewidth=0.2)\n",
    "    plt.title('Test Data')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str('plots/'+nn+'_results.jpeg'), edgecolor='black', dpi=400, transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "#target = target_data.reshape(1, -1, 1)\n",
    "#y_pred = y_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "#   TDNN: every epoch you pass all the matrix input (window)\n",
    "#   RNN: every epoch you pass a timestep and let it run\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-delay neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This is a PyTorch neural network model class that implements a simple TDNN architecture with a\n",
    "specified number of hidden layers and dimensions. \n",
    "'''\n",
    "\n",
    "class TDNN_model(torch.nn.Module):\n",
    "    def __init__(self, window:int, layers:int, input_dim:int, epochs:int, hidden_dim:int): \n",
    "        super(TDNN_model, self).__init__()\n",
    "        \n",
    "        hidden_layers = [] #torch.nn.ModuleList()\n",
    "        # hidden = torch.nn.ModuleList([torch.nn.ReLU(), torch.nn.Linear(hidden_dim, hidden_dim)])\n",
    "        # self.all_layers.append([hidden for i in range(1,layers)])\n",
    "        \n",
    "        for _ in range(layers):\n",
    "            hidden_layers.append(torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "            hidden_layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(window, hidden_dim), #window*input_dim\n",
    "            torch.nn.ReLU(),\n",
    "            *hidden_layers,\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Tanh()\n",
    "            ) \n",
    "        \n",
    "        \n",
    "        self.window=window   \n",
    "        self.epochs=epochs   \n",
    "        \n",
    "                \n",
    "    def train(self, input_data:Tensor, target_data:Tensor, optimizer):\n",
    "        #create sequences of data\n",
    "        timesteps=input_prep(self.window, input_data)\n",
    "        #pass them to the model        \n",
    "        #then to the optimizer\n",
    "        target = torch.tensor(target_data, dtype=torch.float32).reshape(1, -1, 1)\n",
    "        # target=input_prep(self.window, target_data) #self.window\n",
    "        for e in range(1,self.epochs):\n",
    "            # print(\"EPOCH \", e, \"\\n-----------------------------------------------------\")\n",
    "            y_pred = self.model(timesteps)\n",
    "            # print(f'Target shape: {target.shape}, Predicted shape: {y_pred.shape}')\n",
    "            mse = MSELoss()\n",
    "            # y_pred = y_pred.detach().numpy().squeeze()\n",
    "            loss = mse(y_pred, target) \n",
    "            #  (torch.tensor(target_data)).unsqueeze(0).unsqueeze(-1)\n",
    "            # loss = float(loss)\n",
    "            # print(f'Loss: {loss}')\n",
    "        if loss is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            print(f'Loss: {loss.item()} \\n-----------------------------------------------------')\n",
    "            #catch loss and history and get them back to the grid search for storage and evaluation\n",
    "            return(loss.item(), y_pred)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam=torch.optim.Adam()\n",
    "# sgd=torch.optim.SGD()\n",
    "params={\n",
    "    'epochs':[100, 200, 500],\n",
    "    'layers':[1,3,5,10],\n",
    "    'hiddens':[10, 50, 100],\n",
    "    'window':[3,5,10,15],\n",
    "    'lr':[0.0001, 0.001, 0.01],\n",
    "    'opt':['adam', 'sgd']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn='TDNN'\n",
    "grid_search=GS(params, x_train, y_train, nn) \n",
    "best_loss, best_model=grid_search.min_loss, grid_search.best_model\n",
    "tr_ypred = best_model[1]\n",
    "\n",
    "print('best training loss: ', best_loss, '\\n best training model: ', best_model[0])\n",
    "\n",
    "grid_search_val=GS(params, x_val, y_val, nn) \n",
    "best_loss_val, best_model_val=grid_search.min_loss, grid_search.best_model\n",
    "best_params = best_model_val[0]\n",
    "\n",
    "print('Cross validation: \\n best loss on validation: ', best_loss_val, '\\n best model parametrization based on validation: ',best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation:')\n",
    "tdnn = TDNN_model(best_params['window'], best_params['layers'], x_val.shape[0], best_params['epochs'], best_params['hiddens'] )\n",
    "optimizer = GS.get_optimizer(tdnn, best_params['opt'], best_params['lr'])  \n",
    "val_loss, val_ypred=tdnn.train(x_val, y_val,optimizer) \n",
    "# best_loss_val, best_model_val=grid_search.min_loss, grid_search.best_model\n",
    "# grid_search_test=GS(params, x_test, y_test, nn) \n",
    "# best_loss_test, best_model_test=grid_search.min_loss, grid_search.best_model\n",
    "print('Test:')\n",
    "tdnn = TDNN_model(best_params['window'], best_params['layers'], x_test.shape[0], best_params['epochs'], best_params['hiddens'] )\n",
    "optimizer = GS.get_optimizer(tdnn, best_params['opt'], best_params['lr'])  \n",
    "test_loss, test_ypred=tdnn.train(x_test, y_test,optimizer) \n",
    "\n",
    "print('Best loss: \\n validation: ', val_loss, '\\n test: ', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_ypred = tr_ypred.detach().numpy()\n",
    "# test_ypred = test_ypred.detach().numpy()\n",
    "\n",
    "y_train.shape, tr_ypred.shape, y_test.shape, test_ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ypred = tr_ypred.detach().numpy()\n",
    "test_ypred = test_ypred.detach().numpy()\n",
    "tr_ypred.shape, test_ypred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(y_train, tr_ypred.squeeze(), y_test, test_ypred.squeeze(), nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This is a PyTorch neural network model class that implements a simple RNN architecture with a\n",
    "specified number of hidden layers and dimensions. \n",
    "'''\n",
    "\n",
    "class RNN_model(torch.nn.Module):\n",
    "    def __init__(self, input_dim:int, layers:int, epochs:int, hidden_dim:int):\n",
    "        super(RNN_model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, layers, nonlinearity='relu', bias=False, batch_first=True)\n",
    "        self.readout = torch.nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.epochs=epochs\n",
    "    \n",
    "    ############## ORIGINAL ONE ######################\n",
    "    # def forward(self, x):\n",
    "    #     '''\n",
    "    #     x.size = [1, 4000, 1]\n",
    "    #     '''\n",
    "    #     # h0 = torch.zeros(self.layers, x.size(1), self.hidden_dim)\n",
    "    #     # print('element ')\n",
    "    #     h0 = torch.zeros(self.layers, x.size(0), self.hidden_dim)\n",
    "    #     # print('h0: ', h0.shape)\n",
    "\n",
    "    #     # One time step\n",
    "    #     for i in range(x.size(1)):\n",
    "    #         # print(x[:, i, :], x[:, i, :].shape) ###############HERE IS THE ERROR!!!!!!!!!!!!!!!!\n",
    "    #         out, hn = self.rnn(x[:, i, :], h0) ########## x[:, :i, :] IS AN EMPTY TENSOR!!!!!!!!!!!!!!!!!!!! BUT IT WAS YOUR FIRST CHOICE, REMEMBER THAT\n",
    "    #         print(\"out: \", out.shape, \"hn: \", hn.shape)\n",
    "    #     # print('out: ', out.shape)\n",
    "    #     out = self.readout(hn[:, 1, :]) \n",
    "    #     return out\n",
    "    ############### NEW ONE #################\n",
    "    def forward(self, x):\n",
    "        ############# first try #############\n",
    "        # h0 = torch.zeros(self.layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # for i in range(x.size(1)):\n",
    "        #     out, hn = self.rnn(x[:, i, :], h0)\n",
    "        # out = self.readout(hn[:, -1, :])\n",
    "        # return out\n",
    "        \n",
    "        ############ second try ###########\n",
    "        # print(x)\n",
    "        # h0 = torch.zeros(self.layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # for i in range(x.size(1)):\n",
    "        #     # print(x[:, i, :], h0.shape)\n",
    "        #     out, hn = self.rnn(x[:, i, :], h0)\n",
    "        # out = self.readout(hn[:, -1, :])\n",
    "        # return out\n",
    "        output = []\n",
    "        h0 = torch.zeros(self.layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        for i in range(0, x.size(1)):\n",
    "            for layer in range(self.layers):\n",
    "                out, hn = self.rnn(x, h0) #_ al posto di hn\n",
    "                # print('hn:', hn.shape)\n",
    "            out = self.readout(hn[:, -1, :]) #out al posto di hn\n",
    "            output.append(out)\n",
    "        output = torch.stack(out)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, input_data:Tensor, target_data:Tensor, optimizer):\n",
    "        # input_data = input_data#.float()  # Ensure input_data is FloatTensor\n",
    "        # target_data = target_data#.float()  # Ensure target_data is FloatTensor\n",
    "        for e in range(1,self.epochs):\n",
    "            # print(\"EPOCH \", e, \"\\n-----------------------------------------------------\")\n",
    "            y_pred = self.forward(input_data)\n",
    "            # print('y pred: ', y_pred.shape, 'target shape: ', target_data.shape)\n",
    "            mse = MSELoss()\n",
    "            loss = mse(y_pred, target_data)\n",
    "            # loss= round(loss.item(), 4)# !! here you might insert a condition for a certain number of previous losses that if they're all the same (=not learning anymore) you stop\n",
    "            # print(f'Loss: {loss}')\n",
    "        if loss is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()    \n",
    "            print(f'Loss: {loss.item()} \\n-----------------------------------------------------')       \n",
    "            ###catch loss and history and get them back to the grid search for storage and evaluation\n",
    "            return(loss, y_pred)    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam=torch.optim.Adam()\n",
    "# sgd=torch.optim.SGD()\n",
    "params={\n",
    "    'epochs':[100, 200, 500], \n",
    "    'hiddens':[1,2,3,5,10],\n",
    "    'lr':[0.0001, 0.001, 0.01],\n",
    "    'opt':['adam', 'sgd']\n",
    "}\n",
    "\n",
    "nn='RNN'\n",
    "x_tr =  torch.from_numpy(np.reshape(x_train, (1, 4000, 1))).float() \n",
    "y_tr =  torch.from_numpy(np.reshape(y_train, (1, 4000, 1))).float() \n",
    "\n",
    "x_v =  torch.from_numpy(np.reshape(x_val, (1, 1000, 1))).float() \n",
    "y_v =  torch.from_numpy(np.reshape(y_val, (1, 1000, 1))).float() \n",
    "\n",
    "\n",
    "grid_search=GS(params, x_tr, y_tr, nn) \n",
    "best_loss, best_model=grid_search.min_loss, grid_search.best_model\n",
    "# print('best loss: ', best_loss, '\\n best model: ', best_model)\n",
    "print('best training loss: ', best_loss)#, '\\n best training model: ', best_model)\n",
    "grid_search_val=GS(params, x_v, y_v, nn) \n",
    "best_loss_val, best_model_val=grid_search.min_loss, grid_search.best_model\n",
    "best_params = best_model_val[0]\n",
    "print('Cross validation: \\n best loss on validation: ', best_loss_val, '\\n best model parametrization based on validation: ',best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t =  torch.from_numpy(np.reshape(x_test, (1, 5000, 1))).float() \n",
    "y_t =  torch.from_numpy(np.reshape(y_test, (1, 5000, 1))).float() \n",
    "\n",
    "rnn = RNN_model(x_v.size(0), best_params['hiddens'], best_params['epochs'], 10)\n",
    "optimizer = GS.get_optimizer(rnn, best_params['opt'], best_params['lr'])  \n",
    "val_loss, val_ypred=rnn.train(x_v, y_v,optimizer) \n",
    "# best_loss_val, best_model_val=grid_search.min_loss, grid_search.best_model\n",
    "# grid_search_test=GS(params, x_test, y_test, nn) \n",
    "# best_loss_test, best_model_test=grid_search.min_loss, grid_search.best_model\n",
    "rnn = RNN_model(x_t.size(0), best_params['hiddens'], best_params['epochs'], 10)\n",
    "optimizer = GS.get_optimizer(rnn, best_params['opt'], best_params['lr'])  \n",
    "test_loss, test_ypred=rnn.train(x_t, y_t,optimizer) \n",
    "\n",
    "print('Best loss: \\n validation: ', val_loss, '\\n test: ', test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_results(y_tr, best_model[1], y_t, test_ypred, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
