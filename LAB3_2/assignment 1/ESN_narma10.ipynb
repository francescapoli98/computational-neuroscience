{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARMA10 task with Echo State Networks\n",
    "\n",
    "This task consists in predicting the output of a 10-th order non-linear autoregressive moving average (NARMA) system.\n",
    "- [Reference paper](https://doi.org/10.1016/j.neunet.2011.02.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from gridsearch import GS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the system is a sequence of elements 洧논(洧노) randomly chosen according to a uniform distribution over [0, 0.5]. \n",
    "\n",
    "Given the input value 洧논(洧노), the task is to predict the corresponding value of 洧녽(洧노).\n",
    "\n",
    "- Import the dataset from the .csv file *NARMA10.csv*, where the first row represents the input and the second row represents the\n",
    "target output. Different columns represent different time-steps.\n",
    "- Split the data into training (the first 5000 time steps), and test set (remaining time steps). Note that for model selection you will use the data in the training set, with a further split in training (first 4000 samples) and validation (last 1000 samples).\n",
    "    - For the sake of problem understanding, you can try to first visualize the timeseries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = pd.read_csv('../data/NARMA10.csv').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = pd.read_csv('../data/NARMA10.csv', header=None).to_numpy()\n",
    "\n",
    "x = dset[0]\n",
    "y = dset[1]\n",
    "x_test = x[:5000]\n",
    "train = x[5000:] \n",
    "\n",
    "x_train = train[:4000]\n",
    "x_val = train[4000:]\n",
    "\n",
    "y_test = y[:5000]\n",
    "train_y = y[5000:] \n",
    "\n",
    "y_train = train_y[:4000]\n",
    "y_val = train_y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_ts(x_train,x_val,x_test, \"NARMA10 input values\")\n",
    "\n",
    "limit1=int(x_train.shape[0])\n",
    "limit2=int(x_train.shape[0]+x_val.shape[0])\n",
    "limit3=int(limit2+x_test.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12,5), dpi=200)\n",
    "plt.plot(range(0,limit1), x_train, color=\"indigo\", linewidth=0.2, label=\"training set\") \n",
    "plt.plot(range(limit1,limit2), x_val, color=\"purple\", linewidth=0.2, label=\"validation set\") \n",
    "plt.plot(range(limit2,limit3), x_test, color=\"violet\", linewidth=0.2, label=\"test set\") \n",
    "\n",
    "plt.title('NARMA10 input values')\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.grid(color = 'green', linestyle = '--', linewidth = 0.2)\n",
    "#plt.savefig(str('plots/timeseries.jpeg'), edgecolor='black', dpi=400, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement from scratch the code required to initialize, run, train, and evaluate an **Echo State Network**. Your implementation should take into\n",
    "consideration relevant hyper-parametrization of the neural network (e.g., number of reservoir neurons, spectral radius, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# # first test the nn then resume this complex grid\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class GS:\n",
    "    \n",
    "    def __init__(self, parameters:dict, Xset:tuple, Yset:tuple):\n",
    "        self.Xset=Xset\n",
    "        self.Yset=Yset\n",
    "        param_grid = self.grid(self, parameters)\n",
    "        min_loss, best_model = self.search(self, param_grid)\n",
    "        self.min_loss = min_loss\n",
    "        self.best_model = best_model # the best model discovered\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def grid(self, params):\n",
    "        param_names=list(params.keys())\n",
    "        param_values=list(params.values())\n",
    "        param_combinations=list(itertools.product(*param_values))\n",
    "        \n",
    "        param_grid=[]\n",
    "        for combination in param_combinations:\n",
    "            param_grid.append(dict(zip(param_names, combination)))\n",
    "        return param_grid \n",
    "        \n",
    "        ###       !! FROM HERE ON, TO BE CHECKED BETTER AFTER DEVELOPING THE TRAINING FUNCION !!      ###\n",
    "    @staticmethod\n",
    "    def search(self, param_grid:tuple):#, Xset, Yset):\n",
    "        predictions={}  \n",
    "        for pg in param_grid:\n",
    "            model=ESN(\n",
    "                input_dim=self.Xset.shape[1], \n",
    "                reservoir_dim=pg['reservoir'], \n",
    "                omega=pg['omega'], \n",
    "                sr=pg['sr'], \n",
    "                sparsity=pg['sparsity'], \n",
    "                leak=pg['leak']\n",
    "                )\n",
    "            tr_loss, y_pred_tr=model.fit(self.Xset, self.Yset) #also do the predict that actually returns what's requested here\n",
    "            val_loss, y_pred_val=model.predict(self.Xset, self.Yset,y_pred_tr[-1])\n",
    "            predictions[loss]=[pg, y_pred] \n",
    "        \n",
    "        # predictions=sorted(predictions.items())\n",
    "        # min_loss=list(predictions.keys())[0]\n",
    "        # best_model=list(predictions.values())[0]\n",
    "        min_loss = min(predictions.keys())\n",
    "        best_model = predictions[min_loss]\n",
    "        \n",
    "        return min_loss, best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Echo State Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code defines a class called `ESN` which stands for Echo State Network. \n",
    "An Echo State Network is a type of recurrent neural network known for its simplicity and effectiveness in time-series prediction tasks.\n",
    "'''\n",
    "class ESN:\n",
    "    def __init__(self, input_dim, reservoir_dim, omega, sr, sparsity, leak):\n",
    "        self.input_dim = 1\n",
    "        self.reservoir_dim = reservoir_dim\n",
    "        self.output_dim = 1\n",
    "        self.omega = omega\n",
    "        self.sr = sr\n",
    "        self.sparsity = sparsity\n",
    "        self.leak = leak\n",
    "        self.bias = (np.random.rand(reservoir_dim, 1) * 2 - 1) * omega\n",
    "        self.transient=100\n",
    "\n",
    "        # weight initialization\n",
    "        ##W IN WITH SECOND DIMENSION AS INPUT_DIM IS A TEST\n",
    "        self.W_in = (np.random.rand(reservoir_dim,input_dim) * 2 - 1) * omega #Choose the values of W from a random distribution scaled by a hyper-parameter 픨_in\n",
    "        \n",
    "        self.W_res = np.random.rand(reservoir_dim, reservoir_dim) * 2 - 1 \n",
    "        # spectral radius (ro) condition\n",
    "        radius = np.max(np.abs(np.linalg.eigvals(self.W_res)))\n",
    "        self.W_res *= sr / radius\n",
    "        # apply sparsity\n",
    "        mask = np.random.rand(reservoir_dim, reservoir_dim) > self.sparsity\n",
    "        self.W_res[mask] = 0\n",
    "        \n",
    "        self.W_out = np.random.rand(reservoir_dim, self.output_dim) * 2 - 1\n",
    "        \n",
    "        self.state = np.zeros(reservoir_dim)\n",
    "        \n",
    "        \n",
    "    def _update_state(self, input_vector):\n",
    "        pre_activation = np.dot(self.W_in, input_vector) + np.dot(self.W_res, self.state) + self.bias \n",
    "\n",
    "        self.state = np.tanh(pre_activation + self.leak * (np.random.rand(self.reservoir_dim) - 0.5))\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    def fit(self, inputs, target):\n",
    "        states = []\n",
    "        # states.append(self.state)\n",
    "        \n",
    "        # print('INPUTS:', inputs)\n",
    "        for input_vector in inputs:\n",
    "            states.append(self._update_state(input_vector))\n",
    "        \n",
    "        # states = np.vstack(states)\n",
    "        # Add bias term\n",
    "        # states = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        # print(f'target: {target}, states: {states}')\n",
    "\n",
    "        # Discard the transient\n",
    "        # states, target = states[self.transient:], target[self.transient:]\n",
    "        \n",
    "        # Ridge regression (Tikhonov regularization)\n",
    "        reg = 1e-8\n",
    "        #readout\n",
    "        self.W_out = np.dot(np.dot(target.T, states), np.linalg.inv(np.dot(states.T, states) + reg * np.eye(states.shape[1]))).T\n",
    "        \n",
    "        y_pred = states @ self.W_out\n",
    "        return mean_squared_error(target, y_pred), states[-1]\n",
    "\n",
    "        \n",
    "    def predict(self, inputs,target,h0):\n",
    "        states = []\n",
    "        states.append(h0)\n",
    "\n",
    "        for input_vector in inputs:\n",
    "            states.append(self._update_state(input_vector))\n",
    "        # states.append(self._update_state(inputs))\n",
    "        \n",
    "        # states = np.vstack(states)\n",
    "        # Add bias term\n",
    "        # states = np.hstack((states, np.ones((states.shape[0], 1))))\n",
    "        \n",
    "        y_pred=np.dot(states, self.W_out)\n",
    "        loss = mean_squared_error(target, y_pred)\n",
    "\n",
    "        output = states[-1], y_pred\n",
    "        return ((loss,) + output) #if loss is not None else output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = x_train.reshape(-1, 1) #.reshape(-1, 1, 1)\n",
    "Ytrain = y_train.reshape(-1, 1)\n",
    "Xtest = x_test.reshape(-1, 1) #.reshape(-1, 1, 1)\n",
    "Ytest = y_test.reshape(-1, 1)\n",
    "\n",
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'reservoir':[100, 200, 500],\n",
    "    'omega':[0.1, 0.5],\n",
    "    'sr':[0.5, 0.8, 0.9],\n",
    "    'sparsity':[0.5, 0.8, 0.9],\n",
    "    'leak':[0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "grid_search=GS(params, Xtrain, Ytrain) \n",
    "best_loss, best_model=grid_search.min_loss, grid_search.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
