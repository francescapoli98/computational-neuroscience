{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5DsFNYYKgHw"
      },
      "source": [
        "# Bonus-Track Assignment 5: CharRNN, or “The Unreasonable Effectiveness of Recurrent Neural Networks”\n",
        "## Character-level RNN for text generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from torch import cuda, zeros, Tensor, optim, nn\n",
        "from torch.nn import Module, ModuleList, Sequential, Linear, Tanh, MSELoss, RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m37QhAvYm9Cv",
        "outputId": "61695af7-a6ce-4e9a-a744-c774f83fb6b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File downloaded and saved at: c:\\Users\\Francesca\\OneDrive\\Desktop\\cns_labs\\computational-neuroscience\\LAB3_2\\bonus tracks\\shakespeare.txt\n"
          ]
        }
      ],
      "source": [
        "# Read a file and split into lines\n",
        "url = 'https://www.gutenberg.org/files/100/100-0.txt' #iliade: 2199\n",
        "filename = 'shakespeare.txt'\n",
        "path = os.path.join(os.getcwd(), filename)\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    response = requests.get(url)\n",
        "    with open(path, 'wb') as text:\n",
        "        text.write(response.content)\n",
        "\n",
        "print(f'File downloaded and saved at: {path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "corpus length: 195955\n"
          ]
        }
      ],
      "source": [
        "text = open('./shakespeare.txt', \"r\", encoding='utf-8').read().lower()\n",
        "# reduce it a bit (Inferno starts at character 2215)\n",
        "text = text[60:196015]\n",
        "\n",
        "print('corpus length:', len(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKM7CUNZoHCc",
        "outputId": "8223736f-e1cf-4e35-c8dc-9cd14873b634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sequences: 48974\n",
            "Unique characters: 59\n"
          ]
        }
      ],
      "source": [
        "maxlen = 60\n",
        "step = 4\n",
        "\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i:i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "print('Number of sequences:', len(sentences))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4WIKkeDpCb5",
        "outputId": "631a3eb9-b3ce-4d59-86bb-958fc1281711"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((48974, 59), (48974, 60, 59))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape, x.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([48974, 59]), torch.Size([48974, 60, 59]))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "y.shape, x.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class charRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(charRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input_combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1)\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = charRNN(x.shape[2], 128, x.shape[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = preds.detach().numpy().astype('float64')  # Convert to numpy array\n",
        "    preds = np.log(preds) / temperature  # Apply temperature scaling\n",
        "    exp_preds = np.exp(preds)  # Exponentiate the scaled values\n",
        "    preds = exp_preds / np.sum(exp_preds)  # Normalize to get probabilities\n",
        "    probas = np.random.multinomial(1, preds, 1)  # Sample from the probability distribution\n",
        "    return np.argmax(probas)  # Return the index of the highest probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generating text with seed: \"fe in my ‘o lord, sir!’ i see things may\n",
            "serve long, but not\"\n",
            "----------------------- \n",
            "Temperature: 0.25\n",
            "fe in my ‘o lord, sir!’ i see things may\n",
            "serve long, but nottensor([-4.2466, -4.0403, -4.3361, -4.1656, -3.9399, -3.9292, -4.0298, -3.8610,\n",
            "        -4.1162, -4.1162, -4.1162, -4.1656, -4.1273, -4.0104, -4.0582, -3.9777,\n",
            "        -4.0123, -4.1744, -4.1288, -4.0652, -4.4297, -4.1711, -3.8593, -4.2081,\n",
            "        -4.1403, -4.1236, -4.0356, -4.2747, -3.9548, -4.1500, -4.2760, -4.0641,\n",
            "        -3.9929, -4.0407, -4.2349, -4.1519, -4.1743, -4.2008, -3.9810, -4.2581,\n",
            "        -4.0707, -3.7848, -3.8857, -4.0569, -4.1989, -4.1392, -3.9171, -3.9801,\n",
            "        -4.1394, -3.9963, -3.9944, -4.0441, -3.9288, -4.3103, -4.1082, -4.2532,\n",
            "        -3.9844, -4.0980, -3.8486], grad_fn=<SelectBackward>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-4808b2b36866>:3: RuntimeWarning: invalid value encountered in log\n",
            "  preds = np.log(preds) / temperature  # Apply temperature scaling\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "pvals < 0, pvals > 1 or pvals contains NaNs",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-d332226cc78f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Sample the next character\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mnext_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-9-4808b2b36866>\u001b[0m in \u001b[0;36msample\u001b[1;34m(preds, temperature)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exponentiate the scaled values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Normalize to get probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprobas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Sample from the probability distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobas\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Return the index of the highest probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.multinomial\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_common.pyx\u001b[0m in \u001b[0;36mnumpy.random._common._check_array_cons_bounded_0_1\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: pvals < 0, pvals > 1 or pvals contains NaNs"
          ]
        }
      ],
      "source": [
        "\n",
        "# Select a text seed at random\n",
        "start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "generated_text = text[start_index:start_index + maxlen]\n",
        "print('--- Generating text with seed: \"' + generated_text + '\"')\n",
        "\n",
        "# Try a range of different temperatures\n",
        "for temperature in [0.25, 0.5, 0.8, 1, 1.5]:\n",
        "    print('----------------------- \\nTemperature:', temperature)\n",
        "    sys.stdout.write(generated_text)\n",
        "\n",
        "    # Initialize the hidden state\n",
        "    hidden = model.initHidden()\n",
        "\n",
        "    # Generate 400 characters starting from the seed text\n",
        "    for i in range(400):\n",
        "        # One-hot encode the characters generated so far\n",
        "        sampled = torch.zeros(1, len(chars))\n",
        "        for char in generated_text:\n",
        "            sampled[0, char_indices[char]] = 1.0\n",
        "\n",
        "        # Forward pass through the model\n",
        "        sampled = sampled.to(hidden.device)\n",
        "        output, hidden = model.forward(sampled, hidden)\n",
        "\n",
        "        # Sample the next character\n",
        "        next_index = sample(output[0], temperature)\n",
        "        next_char = chars[next_index]\n",
        "\n",
        "        # Append the newly generated character\n",
        "        generated_text += next_char\n",
        "        generated_text = generated_text[1:]\n",
        "\n",
        "        sys.stdout.write(next_char)\n",
        "        sys.stdout.flush()\n",
        "    sys.stdout.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
